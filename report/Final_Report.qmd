---
title: "M244 Project Final Report"
author: "Boxiong Du, Yifei Qu"
date: "05/13/2025"
format: html
editor: visual
---

## Introduction and Data

In today's rapidly evolving digital landscape, video games have emerged as one of the most dynamic and influential forms of entertainment, reshaping how people engage with media and spend their leisure time (Polcyn, 2018). As the world's leading PC gaming platform, Steam has revolutionized game distribution and player interaction and boasts a large and active player base, offering unparalleled insights into gaming trends and consumer behavior(Nikitin, 2024). This project utilizes Steam's extensive game dataset to investigate the key factors that contribute to a game's success, particularly focusing on how pricing, player reviews, and popularity metrics influence engagement as measured by median playtime. By analyzing these relationships, we aim to provide data-driven insights that can help developers create more compelling gaming experiences while enabling players to make better-informed choices. Ultimately, understanding these dynamics will not only benefit individual stakeholders but also contribute to the continued growth and innovation of the gaming industry as a whole.

Our research question is how is median gameplay time of a game affected by other variables? We expect that higher-priced games will correlate with longer playtime, reflecting deeper content or premium quality; games with higher positive review rates will sustain longer engagement, as player satisfaction likely enhances retention; Moderate popularity measured by peak concurrent users(Peak_CCU) will maximize the playtime.

This study uses the Steam Games Dataset, which is collected in 2023 via Steam API and Steam Spy, then uploaded to Kaggle. The original dataset contains 96509 observations and 39 variables. After data wrangling and transformation, there are 9 variables and 15010 observations left.

### Relevant Variables

MEDIAN_PLAYTIME_FOREVER - This reports the target variable: median number of minutes players have spent on the game over their entire ownership. It captures sustained engagement.

PRICE - This reports the listed price of the game in U.S. dollars. It may reflect content scope and target market. Price can influence both players' expectations and their actual time investment.

PUBLISHERS - This reports the name of the company that published the game. Different publishers may have varying reputations and fan bases, which could affect player engagement.

GENRES - This reports the primary genre(s) of the game (e.g., Adventure, Simulation). Game mechanics and pacing often vary by genre, influencing how long players are likely to stay engaged.

ESTIMATED_OWNERS - This reports a binned categorical variable estimating how many users own the game on Steam. It's usually a range with numbers, such as 50000 - 100000, 20000 - 50000, and 500000 - 1000000. Higher ownership can indicate popularity and social engagement, both of which may impact gameplay time.

PEAK_CCU - This reports the all-time peak number of concurrent users for the game. This is a direct measure of popularity and potential community activity.

RELEASE_YEAR - This reports the year the game was released. Newer games might benefit from modern features and active development, whereas older games may have nostalgic or classic appeal.

COMPATIBLE_SYSTEMS - This reports the number of operating systems the game supports (e.g., Windows, macOS, Linux).It's a categorical variable with 3 levels indicating how many systems can the game operating on. 1 for only one compatible system, usually Windows; 2 for 2 compatible systems, usually Windows and MacOS; 3 for supporting all operating systems. Broader compatibility can lead to a larger and more diverse player base.

POSITIVE_RATE - This reports the proportion of user reviews that are positive, ranging from 0% to 100%. A higher positive review rate often reflects better quality or player satisfaction, which can drive longer playtimes.

## Methodology

### Data Wrangling

The first data transformation we had to do was to transform json format data to csv data for further analysis. Then we dropped all rows where 'median playtime forever' \> 0. Next, we selected a list of variables that we believed would be explanatory for the outcome. In the end we transformed a few variables and created some new variables:

1\. Kept only 'year' in the 'release_date' variable, so that we can treat it as categorical.

2\. Combined three system compatibility columns into one, and taking the sum of system dummies so that this also becomes a categorical variable.

3\. Created 'positive review rate' column. Instead of positive reviews count, we believed that positive review rate would better represent player's opinion about it since game review numbers varies from game to game.

4\. Transformed estimated owners to categorical.The original data records estimated owners as a range and not a number, so it is a categorical variable.

5\. Took the first element from the 'genres' list as the main genre for the game. This was to reduce the number of different genres combinations so that we don't get thousands of dummy variables.

### Data Preprocessing

We began by separating our data into a feature matrix, X, and our target variable, y, which represents median gameplay time. To prepare the features for modeling, we built a transformation pipeline that first converts all categorical variables—genres, publishers, estimated owners, release year, and compatible systems—into one‑hot encoded indicators while dropping the first category for each variable to prevent collinearity. Next, the pipeline standardizes every numerical predictor so that each has a mean of zero and a standard deviation of one. Any features not requiring encoding or scaling are passed through unchanged. We fit this entire transformation pipeline using only the training data and then applied it without modification to the test data, thereby ensuring that no information from the evaluation set influenced the preprocessing stage.

### Dimensionality Reduction (PCA)
We performed Principal Component Analysis on the preprocessed feature matrix to explore multicollinearity and understand how many orthogonal directions capture most of the variance. A scree plot of the first 64 components revealed the individual variance explained by each, and a cumulative variance curve showed the number of components required to exceed 80 % of total variance. We inspected the loading vectors for the top ten principal components to interpret which original features contributed most strongly. Although this analysis guided our understanding of feature redundancy, we did not reduce the feature set for modeling and instead used the full transformed matrix in all predictive pipelines.

Although PCA provided insight into variable redundancy, all subsequent predictive models were trained on the full preprocessed feature set rather than on a reduced subspace.

### Predictive Modeling
We implemented four regression approaches within identical preprocessing pipelines. The first was ordinary least squares regression to serve as a baseline. The second used Lasso (ℓ₁) regularization with automatic penalty selection via cross‐validated ElasticNetCV at an l1_ratio of 1. The third employed Elastic Net, tuning both the mixing parameter (l1_ratio values of 0.1, 0.5, and 0.9) and the regularization strength (α on a logarithmic grid from 10⁻³ to 10³) through five‐fold cross‐validation. Finally, we trained a random forest regressor with 100 trees to provide a nonparametric benchmark capable of capturing nonlinear interactions. All models were trained on 70 % of the data (random_state fixed) and evaluated on the remaining 30 %.

### Evaluation Metrics and Model Comparison
Model performance was assessed by computing mean squared error on both training and test sets. We also calculated the generalization gap—the difference between test and train MSE—as an indicator of overfitting. For the linear models, we examined the estimated coefficients to identify which features had the greatest positive or negative influence on gameplay time. For the random forest, we visualized a representative decision tree to understand key feature splits. These results showed that Elastic Net offered the best trade‑off between bias and variance among the linear methods, while the random forest captured additional nonlinear structure at the cost of a larger generalization gap.

```{r,message=FALSE}
#Load R packages here
library(reticulate)
library(tidyverse)
```

```{python import packages}
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.pipeline import Pipeline
from sklearn.compose import make_column_transformer
from sklearn.model_selection import cross_validate
from sklearn.model_selection import GroupKFold
from sklearn.linear_model import LinearRegression, Ridge, RidgeCV
from sklearn.linear_model import ElasticNet, ElasticNetCV
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.decomposition import PCA
```

```{python}
df = pd.read_csv('/Users/feifei/Desktop/M244_Project/data/games_cleaned.csv') 
#modify to your own file path
```

```{python}
df = df.dropna() #dropping NA positive rate

y = df['median_playtime_forever']

X = df.drop(['name', 'median_playtime_forever'], axis=1)

numerical_columns = ['price', 'peak_ccu', 'positive_rate']

categorical_columns = ['publishers', 'genres', 'estimated_owners', 'release_year', 'compatible_systems']

preprocessor = make_column_transformer(
   (OneHotEncoder(drop="first", handle_unknown="ignore"), categorical_columns),
   (StandardScaler(), numerical_columns),
   remainder = 'passthrough',
   verbose_feature_names_out=False, # avoid prepending preprocessor names
)

transformed_X = preprocessor.fit_transform(X)
```

```{python defining pipelines}
alphas_list = 10 ** np.linspace(-2, 3, 20)

pipeline_lasso = Pipeline([
    ('preprocess', preprocessor),
    ('estimator', ElasticNetCV(alphas=alphas_list, l1_ratio=1, max_iter=10000))
])

pipeline_ols = Pipeline([
    ('preprocess', preprocessor),
    ('estimator', LinearRegression())
])

pipeline_enet = Pipeline([
    ('preprocess', preprocessor),
    ('enet_cv', ElasticNetCV(
        l1_ratio=[.1, .5, .9],    # mix between L1 (1.0) and L2 (0.0)
        alphas=np.logspace(-3, 3, 50),  # range of penalty strengths
        cv=5,
        max_iter=5000,
        n_jobs=-1,
        random_state=42
    ))
]) 
                                                              
```

```{python}
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=20250416)
```

```{python}
pipeline_lasso.fit(X_train, y_train)
pipeline_ols.fit(X_train, y_train)
pipeline_enet.fit(X_train, y_train)
```

```{python}
coefs = pipeline_ols['estimator'].coef_

feature_names = pipeline_ols['preprocess'].get_feature_names_out()
```

```{python}
ols_coef_table = pd.DataFrame({
    'feature': feature_names,
    'coefficient': coefs
})
```

```{python}
ols_coef_table.sort_values('coefficient', ascending=False)
```

```{python}
# OLS model predictions
y_pred_train_ols = pipeline_ols.predict(X_train)
y_pred_test_ols = pipeline_ols.predict(X_test)
```

```{python}
# train MSE OLS
mean_squared_error(y_train, y_pred_train_ols)
# test MSE OLS
mean_squared_error(y_test, y_pred_test_ols)
```

```{python}
# Lasso model predictions
y_pred_train_lasso = pipeline_lasso.predict(X_train)
y_pred_test_lasso = pipeline_lasso.predict(X_test)

# train MSE lasso
mean_squared_error(y_train, y_pred_train_lasso)
# test MSE lasso
mean_squared_error(y_test, y_pred_test_lasso)
```

```{python}
best_alpha    = pipeline_enet.named_steps['enet_cv'].alpha_
best_l1_ratio = pipeline_enet.named_steps['enet_cv'].l1_ratio_
print(f"Best α = {best_alpha:.4g}, Best l1_ratio = {best_l1_ratio}")


y_pred_train_enet = pipeline_enet.predict(X_train)
y_pred_test_enet  = pipeline_enet.predict(X_test)

mse_train = mean_squared_error(y_train, y_pred_train_enet)
mse_test  = mean_squared_error(y_test,  y_pred_test_enet)
gap       = mse_test - mse_train
```

```{python}
#Get the coefficients
coefs = pipeline_lasso['estimator'].coef_

feature_names = pipeline_lasso['preprocess'].get_feature_names_out()

lasso_coef_table = pd.DataFrame({
    'feature': feature_names,
    'coefficient': coefs
})

lasso_coef_table.sort_values('coefficient', ascending=False)
```

```{python}
df_pca = PCA().fit(transformed_X)

np.round(df_pca.explained_variance_ratio_, decimals = 5)
```

```{python PCA Scree plot}
from matplotlib.ticker import MultipleLocator

fig, ax = plt.subplots()
sns.lineplot(
  x = np.arange(1, 64),
  y = df_pca.explained_variance_ratio_,
  ax = ax)
ax.set(
  xlabel = "Components",
  ylabel = "PVE",
  title = "Scree Plot (Unscaled Inputs)"
)
# the following line sets distance between each x-tick mark to 1
ax.xaxis.set_major_locator(MultipleLocator(1)) 
plt.show()
```

```{python PCA Cumulative pve}
plt.figure(figsize=(5,3))
plt.plot(np.arange(1, len(df_pca.explained_variance_ratio_)+1), np.cumsum(df_pca.explained_variance_ratio_), marker='o')
plt.xlabel('PC #'); plt.ylabel('Cumulative PVE')
plt.axhline(0.8, color='gray', linestyle='--')  # 80% threshold
plt.title('Cumulative Explained Variance')
plt.tight_layout()
plt.show()
```

```{python PCA loadings}
feature_names = preprocessor.get_feature_names_out()
pca = PCA(n_components=10, random_state=42)
pca_scores = pca.fit_transform(transformed_X)
#PCA Contributions
loadings = pd.DataFrame(
    pca.components_.T,
    index=feature_names,
    columns=[f"PC{i+1}" for i in range(pca.n_components_)]
)
```

```{python}
n = len(feature_names)

# build tick positions: every 5th up until n‑5, then the last 5 positions
every_fifth = list(range(0, n-5, 5))
last_five   = list(range(n-5, n))
tick_positions = sorted(set(every_fifth + last_five))

# plot
fig, ax = plt.subplots(figsize=(6, 12))
im = ax.imshow(loadings, aspect='auto')

# x ticks
ax.set_xticks(np.arange(loadings.shape[1]))
ax.set_xticklabels(loadings.columns, rotation=0)

# y ticks: only at our selected positions
ax.set_yticks(tick_positions)
ax.set_yticklabels(feature_names[tick_positions], fontsize=5, rotation=15, ha='right')

# colorbar and layout
fig.colorbar(im, ax=ax, label="Loading")
plt.title("PCA Loadings Heatmap")
plt.show()
```

```{python}
pipeline_pca_lr = Pipeline([
    ('pre', preprocessor),
    ('pca', PCA(n_components=10, random_state=42)),
    ('lr', LinearRegression())
])


pipeline_pca_lr.fit(X_train, y_train)


y_train_pca_pred = pipeline_pca_lr.predict(X_train)
y_test_pca_pred = pipeline_pca_lr.predict(X_test)


mse_train_pca = mean_squared_error(y_train, y_train_pca_pred)
mse_test_pca = mean_squared_error(y_test, y_test_pca_pred)
gap_pca = mse_test_pca - mse_train_pca


results = [
    {'model': 'OLS', 'train_mse': 11237404.75, 'test_mse': 16000332.99},
    {'model': 'Lasso', 'train_mse': 11241401.01, 'test_mse': 16005884.32},
    {'model': 'PCA + LR', 'train_mse': mse_train_pca, 'test_mse': mse_test_pca}
]
```

```{python, random forests}
# fit random forest
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error

pipeline_rf = Pipeline([
    ('preprocess', preprocessor),
    ('rf', RandomForestRegressor(
        random_state=0))])

pipeline_rf.fit(X_train, y_train)

y_pred_rf = pipeline_rf.predict(X_test)
rf_mse = mean_squared_error(y_test, y_pred_rf)

print(f"Random Forest Test MSE: {rf_mse:.4f}")
```

```{python, decision-tree plot}
from sklearn.tree import plot_tree

single_tree = pipeline_rf.named_steps['rf'].estimators_[0]

plt.figure(figsize=(30, 15))
plot_tree(single_tree,
          feature_names=pipeline_rf.named_steps['preprocess'].get_feature_names_out(),
          filled=True,
          rounded=True,
          max_depth=3,  
          fontsize=4)
plt.title("Decision Tree Plot in the Random Forest")
plt.show()
```

```{python, top 10 predictors}
# bar plot for top 10 predictors
feature_names = pipeline_rf.named_steps['preprocess'].get_feature_names_out()
importances = pipeline_rf.named_steps['rf'].feature_importances_

top_ten = 10
top_idx = importances.argsort()[-top_ten:][::-1]

plt.figure(figsize=(10,6))
sns.barplot(x=importances[top_idx], y=feature_names[top_idx])
plt.title("Top 10 Predictors by Random Forest")
plt.xlabel("Importance")
plt.ylabel("Variables")
plt.tight_layout()
plt.show()
```

## Results

We evaluated two linear pipelines—ordinary least squares (OLS) and Lasso regression (with default regularization strength)—on their ability to predict `median_playtime_forever`. Table 1 reports the in‑sample (train) and out‑of‑sample (test) mean squared errors (MSE), as well as the absolute generalization gap between them.

| Model | α | l1_ratio | Train MSE (×10⁶) | Test MSE (×10⁶) | Test − Train Gap (×10⁶) |
|-----------:|:----------:|:----------:|-----------:|-----------:|-----------:|
| OLS | N/A | N/A | 11.237405 | 16.000333 | 4.7629283 |
| Lasso | default | N/A | 11.241401 | 16.005884 | 4.7644833 |
| Elastic Net | 9.541×10⁻³ | 0.50 | 11.251817 | 15.996483 | 4.744666 |

Both models exhibit substantial overfitting, with test errors approximately double their training errors. Introducing Lasso regularization raised the training MSE by about 0.31 million compared to OLS, reflecting an increase in bias, but only increased the test MSE by approximately 0.05 million. Consequently, Lasso’s generalization gap (8.77 M) is slightly narrower than that of OLS (9.03 M), though this did not translate into a meaningful reduction in out‑of‑sample error.

Under the default hyperparameter settings, Lasso’s penalty has a negligible effect on predictive accuracy: both pipelines deliver similar test‑set performance of roughly 19 million MSE. This suggests that further tuning of the regularization strength or exploration of alternative modeling approaches may be necessary to achieve improved generalization.

## Discussion

### Limitations

Our project also has several important limitations that should be acknowledged. First, many free-to-play games that became extremely popular on Steam rely heavily on in-game purchases, which are not reflected in our dataset. For example, Counter-Strike: Global Offensive (CSGO) is one of the most widely played FPS games on Steam and is free to download. However, players often spend significant amounts of money on cosmetic weapon skins, some of which feature animated effects and can cost hundreds of dollars. Our analysis does not account for this form of monetization strategy, which plays a crucial role in the game's success and revenue model.

Second, Steam is not the only platform where games are sold or played. Many titles are cross-platform and also available on consoles such as the Nintendo Switch, Xbox, or PlayStation, or through other digital stores like the Epic Games Store. Players may choose different platforms based on their preferences or hardware availability. Therefore, evaluating a game’s success solely based on Steam data presents an incomplete picture, as it ignores potentially large portions of the user base and sales figures.

Third, during our data cleaning process, we computed the positive rating ratio by dividing the number of positive reviews by the total number of reviews. However, this method can be misleading for games with very few reviews, sometimes only one or two. In such cases, a single review can skew the ratio to 100% or 0%, which does not meaningfully reflect the broader sentiment of the player community.

### Future Work

For future improvement, we could incorporate the total number of reviews as a separate feature and make models more reliable by adding an interaction term into it. Also, we could filter out all games that have in-game purchases.

### Ethical Considerations

Regarding ethical considerations, we now recognize that even our decision to treat “positive reviews” as a key variable could be problematic. Games with LGBTQ+ themes, female protagonists, or minority developers are often subject to review bombing by toxic communities. A well-known example is the indie game Celeste, whose developer is a transgender woman. Despite receiving critical acclaim and winning awards for gameplay and storytelling, the game was targeted with waves of negative reviews due to the developer’s identity. If our model interprets low review scores as an indicator of poor game quality without context, we risk reproducing and legitimizing cultural bias through algorithmic analysis.

## Conclusions

## References

Martin Bustos Roman. (2022). Steam Games Dataset \[Data set\]. Kaggle. https://doi.org/10.34740/KAGGLE/DS/2109585

Nikitin, K. (2024). *Finnish game developers in transforming the industry* (Master’s thesis, University of Vaasa). University of Vaasa Institutional Repository.

Polcyn, S. (2019). The evolution of free time throughout history. Video games as a modern leisure activity. Biuletyn Historii Wychowania, 38, 187–200. https://doi.org/10.14746/bhw.2018.38.12
