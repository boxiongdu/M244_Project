---
title: "M244 Project Final Report"
author: "Boxiong Du, Yifei Qu"
date: "05/13/2025"
format: html
editor: visual
---

## Introduction and Data

In today's rapidly evolving digital landscape, video games have emerged as one of the most dynamic and influential forms of entertainment, reshaping how people engage with media and spend their leisure time (Polcyn, 2018). As the world's leading PC gaming platform, Steam has revolutionized game distribution and player interaction and boasts a large and active player base, offering unparalleled insights into gaming trends and consumer behavior(Nikitin, 2024). This project utilizes Steam's extensive game dataset to investigate the key factors that contribute to a game's success, particularly focusing on how pricing, player reviews, and popularity metrics influence engagement as measured by median playtime. By analyzing these relationships, we aim to provide data-driven insights that can help developers create more compelling gaming experiences while enabling players to make better-informed choices. Ultimately, understanding these dynamics will not only benefit individual stakeholders but also contribute to the continued growth and innovation of the gaming industry as a whole.

Our research question is how is median gameplay time of a game affected by other variables? We expect that higher-priced games will correlate with longer playtime, reflecting deeper content or premium quality; games with higher positive review rates will sustain longer engagement, as player satisfaction likely enhances retention; Moderate popularity measured by peak concurrent users(Peak_CCU) will maximize the playtime.

This study uses the Steam Games Dataset, which is collected in 2023 via Steam API and Steam Spy, then uploaded to Kaggle. The original dataset contains 96509 observations and 39 variables. After data wrangling and transformation, there are 9 variables and 15010 observations left.

### Relevant Variables

We will include the following variables in our models:
MEDIAN_PLAYTIME_FOREVER - This reports the target variable: median number of minutes players have spent on the game over their entire ownership. It captures sustained engagement.

PRICE - This reports the listed price of the game in U.S. dollars. It may reflect content scope and target market. Price can influence both players' expectations and their actual time investment.

PUBLISHERS - This reports the name of the company that published the game. Different publishers may have varying reputations and fan bases, which could affect player engagement.

GENRES - This reports the primary genre(s) of the game (e.g., Adventure, Simulation). Game mechanics and pacing often vary by genre, influencing how long players are likely to stay engaged.

ESTIMATED_OWNERS - This reports a binned categorical variable estimating how many users own the game on Steam. It's usually a range with numbers, such as 50000 - 100000, 20000 - 50000, and 500000 - 1000000. Higher ownership can indicate popularity and social engagement, both of which may impact gameplay time.

PEAK_CCU - This reports the all-time peak number of concurrent users for the game. This is a direct measure of popularity and potential community activity.

RELEASE_YEAR - This reports the year the game was released. Newer games might benefit from modern features and active development, whereas older games may have nostalgic or classic appeal.

COMPATIBLE_SYSTEMS - This reports the number of operating systems the game supports (e.g., Windows, macOS, Linux).It's a categorical variable with 3 levels indicating how many systems can the game operating on. 1 for only one compatible system, usually Windows; 2 for 2 compatible systems, usually Windows and MacOS; 3 for supporting all operating systems. Broader compatibility can lead to a larger and more diverse player base.

POSITIVE_RATE - This reports the proportion of user reviews that are positive, ranging from 0% to 100%. A higher positive review rate often reflects better quality or player satisfaction, which can drive longer playtimes.

## Methodology
```{r import packages r}
library(reticulate)
```

```{python import packages}
#| echo: false
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.pipeline import Pipeline
from sklearn.compose import make_column_transformer
from sklearn.model_selection import cross_validate
from sklearn.model_selection import GroupKFold
from sklearn.linear_model import LinearRegression, Ridge, RidgeCV
from sklearn.linear_model import ElasticNet, ElasticNetCV
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.decomposition import PCA
```
### Data Wrangling

```{python}
#| echo: false
df = pd.read_csv('/Users/feifei/Desktop/M244_Project/data/games_cleaned.csv') 
#modify to your own file path
```

```{python data wrangling}
#| echo: false
df = df.dropna() #dropping NA positive rate

y = df['median_playtime_forever']

X = df.drop(['name', 'median_playtime_forever'], axis=1)

numerical_columns = ['price', 'peak_ccu', 'positive_rate']

categorical_columns = ['publishers', 'genres', 'estimated_owners', 'release_year', 'compatible_systems']

preprocessor = make_column_transformer(
   (OneHotEncoder(drop="first", handle_unknown="ignore"), categorical_columns),
   (StandardScaler(), numerical_columns),
   remainder = 'passthrough',
   verbose_feature_names_out=False, # avoid prepending preprocessor names
)

transformed_X = preprocessor.fit_transform(X)
```

The first data transformation we had to do was to transform json format data to csv data for further analysis. Then we dropped all rows where 'median playtime forever' \> 0. Next, we selected a list of variables that we believed would be explanatory for the outcome. In the end we transformed a few variables and created some new variables:

1\. Kept only 'year' in the 'release_date' variable, so that we can treat it as categorical.

2\. Combined three system compatibility columns into one, and taking the sum of system dummies so that this also becomes a categorical variable.

3\. Created 'positive review rate' column. Instead of positive reviews count, we believed that positive review rate would better represent player's opinion about it since game review numbers varies from game to game.

4\. Transformed estimated owners to categorical.The original data records estimated owners as a range and not a number, so it is a categorical variable.

5\. Took the first element from the 'genres' list as the main genre for the game. This was to reduce the number of different genres combinations so that we don't get thousands of dummy variables.

### Highlights from EDA

<<<<<<< HEAD
```{python}
#| echo: false
#| message: false
#| warning: false
corr_matrix = df[numerical_columns + ['median_playtime_forever']].corr()
plt.figure(figsize=(10, 6))
sns.heatmap(corr_matrix, annot=True, cmap="coolwarm")
plt.title("Figure 1: Correlation Between Numerical Vairables and Playtime")
plt.show()
```
In figure 1, the correlation matrix between numerical variables and playtime shows that price, peak concurrent users, and positive review rates all have a positive correlation with median playtime of games, 0.057, 0.014, and 0.02 respectively, but very weak.

```{python}
#| echo: false
#| message: false
#| warning: false
yearly_trend = df.groupby('release_year')['median_playtime_forever'].mean()

plt.figure(figsize=(10, 5))
sns.lineplot(x=yearly_trend.index, y=yearly_trend.values)
plt.title("Figure 2: Average Median Playtime by Release Year")
plt.xlabel("Release Year")
plt.ylabel("Average Median Playtime")
plt.show()
```
Figure 2 shows that average median playtime increases over time until around 2019, then drops sharply post-2020, indicating that that release year itself affects playtime, possibly due to changes in game design trends, COVID-19 increasing playtime for games released around 2019–2020, and market saturation or shorter gameplay loops in newer titles.

```{python}
#| echo: false
#| message: false
#| warning: false
import matplotlib.patches as mpatches

cat_cols = ['genres', 'estimated_owners']
figure_labels = ['Figure 3', 'Figure 4']  # Add more if needed

for i, col in enumerate(cat_cols):
    plt.figure(figsize=(10, 6))
    
    top_vals = df[col].value_counts().nlargest(10).index
    data = df[df[col].isin(top_vals)]
    
    palette = sns.color_palette("tab10", n_colors=10)
    color_dict = dict(zip(top_vals, palette))
    
    sns.boxplot(data=data, x=col, y='median_playtime_forever',
                palette=color_dict)
    
    plt.xticks(rotation=45)
    plt.ylim(0, 10000)
    plt.title(f"{figure_labels[i]}: Median Playtime by Top {col} Categories")
    plt.xlabel(col)
    plt.ylabel("Median Playtime (minutes)")
    
    handles = [mpatches.Patch(color=color_dict[cat], label=cat) for cat in top_vals]
    plt.legend(handles=handles, title=col, bbox_to_anchor=(1.05, 1), loc='upper left')
    
    plt.tight_layout()
    plt.show()
```
In figure 3, genres clearly differ in how long players tend to engage with the game that Simulation and RPG games have higher medians and wider IQRs, suggesting longer, more immersive gameplay. Casual and Free to Play games show lower medians, fitting expectations for shorter, more repetitive sessions. In figure 4, the boxplot of median gameplay by top Estimated_owners category shows that games with more owners tend to have higher playtime, especially in the upper bins from 5M to 20M.

### Data Preprocessing
We began by separating our data into a feature matrix, X, and our target variable, y, which represents median gameplay time. To prepare the features for modeling, we built a transformation pipeline that first converts all categorical variables—genres, publishers, estimated owners, release year, and compatible systems—into one‑hot encoded indicators while dropping the first category for each variable to prevent collinearity. Next, the pipeline standardizes every numerical predictor so that each has a mean of zero and a standard deviation of one. Any features not requiring encoding or scaling are passed through unchanged. We fit this entire transformation pipeline using only the training data and then applied it without modification to the test data, thereby ensuring that no information from the evaluation set influenced the preprocessing stage.

### Dimensionality Reduction (PCA)
We performed Principal Component Analysis on the preprocessed feature matrix to explore multicollinearity and understand how many orthogonal directions capture most of the variance. A scree plot of the first 64 components revealed the individual variance explained by each, and a cumulative variance curve showed the number of components required to exceed 80 % of total variance. We inspected the loading vectors for the top ten principal components to interpret which original features contributed most strongly. Although this analysis guided our understanding of feature redundancy, we did not reduce the feature set for modeling and instead used the full transformed matrix in all predictive pipelines.

Although PCA provided insight into variable redundancy, all subsequent predictive models were trained on the full preprocessed feature set rather than on a reduced subspace.

### Predictive Modeling
We implemented four regression approaches within identical preprocessing pipelines. All models were trained on 70 % of the data (random_state fixed) and evaluated on the remaining 30 %. The first was ordinary least squares regression to serve as a baseline. The second used Lasso (ℓ₁) regularization with automatic penalty selection via cross‐validated ElasticNetCV at an l1_ratio of 1. The third employed Elastic Net, tuning both the mixing parameter (l1_ratio values of 0.1, 0.5, and 0.9) and the regularization strength (α on a logarithmic grid from 10⁻³ to 10³) through five‐fold cross‐validation. Finally, we trained a random forest regressor with 100 trees to provide a nonparametric benchmark capable of capturing nonlinear interactions. By combining the predictions of multiple decision trees trained on different subsets of the data and features, this model mitigates overfitting issue and improves accuracy. 
```{python}
#| echo: false
#| message: false
#| warning: false

alphas_list = 10 ** np.linspace(-2, 3, 20)

pipeline_lasso = Pipeline([
    ('preprocess', preprocessor),
    ('estimator', ElasticNetCV(alphas=alphas_list, l1_ratio=1, max_iter=10000))
])

pipeline_ols = Pipeline([
    ('preprocess', preprocessor),
    ('estimator', LinearRegression())
])

pipeline_enet = Pipeline([
    ('preprocess', preprocessor),
    ('enet_cv', ElasticNetCV(
        l1_ratio=[.1, .5, .9],    # mix between L1 (1.0) and L2 (0.0)
        alphas=np.logspace(-3, 3, 50),  # range of penalty strengths
        cv=5,
        max_iter=5000,
        n_jobs=-1,
        random_state=42
    ))
]) 
                                                              
```

```{python}
#| echo: false
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=20250416)
```

```{python}
#| echo: false
pipeline_lasso.fit(X_train, y_train)
pipeline_ols.fit(X_train, y_train)
pipeline_enet.fit(X_train, y_train)
```

```{python OLS fit}
#| echo: false
# OLS model predictions
y_pred_train_ols = pipeline_ols.predict(X_train)
y_pred_test_ols = pipeline_ols.predict(X_test)

ols_mse_train = mean_squared_error(y_train, y_pred_train_ols)
# test MSE OLS
ols_mse_test = mean_squared_error(y_test, y_pred_test_ols)
```


```{python Lasso fit}
#| echo: false
# Lasso model predictions
y_pred_train_lasso = pipeline_lasso.predict(X_train)
y_pred_test_lasso = pipeline_lasso.predict(X_test)

# train MSE lasso
lasso_mse_train = mean_squared_error(y_train, y_pred_train_lasso)
# test MSE lasso
lasso_mse_test = mean_squared_error(y_test, y_pred_test_lasso)
```

```{python Elastic net fit}
#| echo: false

# 3. Inspect chosen hyperparameters
best_alpha    = pipeline_enet.named_steps['enet_cv'].alpha_
best_l1_ratio = pipeline_enet.named_steps['enet_cv'].l1_ratio_
print(f"Best α = {best_alpha:.4g}, Best l1_ratio = {best_l1_ratio}")


y_pred_train_enet = pipeline_enet.predict(X_train)
y_pred_test_enet  = pipeline_enet.predict(X_test)

enet_mse_train = mean_squared_error(y_train, y_pred_train_enet)
enet_mse_test  = mean_squared_error(y_test,  y_pred_test_enet)
gap       = enet_mse_test - enet_mse_train
```

```{python, random forests}
#| echo: false
# fit random forest
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error

pipeline_rf = Pipeline([
    ('preprocess', preprocessor),
    ('rf', RandomForestRegressor(
        random_state=0))])

pipeline_rf.fit(X_train, y_train)

y_pred_rf = pipeline_rf.predict(X_test)
rf_mse = mean_squared_error(y_test, y_pred_rf)
```

```{python, decision-tree plot}
#| echo: false
#| fig-width: 15
#| fig-height: 8
from sklearn.tree import plot_tree

single_tree = pipeline_rf.named_steps['rf'].estimators_[0]

plt.figure(figsize=(30, 15), dpi = 200)
plot_tree(single_tree,
          feature_names=pipeline_rf.named_steps['preprocess'].get_feature_names_out(),
          filled=True,
          rounded=True,
          max_depth=2,  
          fontsize=12)
plt.title("Fifure 4: Decision Tree Plot in the Random Forest")
plt.show()
```
Figure 4 illustrates the structure of a single decision tree from the random forest with a limit to the top three levels for clarity. It shows how the model splits based on features such as price, release year, and game genre to make predictions, which visualizes model's internal decision-making process and supports interpretability and feature analysis.

### Evaluation Metrics and Model Comparison
Model performance was assessed by computing mean squared error on both training and test sets. We also calculated the generalization gap—the difference between test and train MSE—as an indicator of overfitting. For the linear models, we examined the estimated coefficients to identify which features had the greatest positive or negative influence on gameplay time. For the random forest, we visualized a representative decision tree to understand key feature splits. These results showed that Elastic Net offered the best trade‑off between bias and variance among the linear methods, while the random forest captured additional nonlinear structure at the cost of a larger generalization gap.


## Results

```{python}
#| echo: false
#Get the coefficients
coefs = pipeline_lasso['estimator'].coef_

feature_names = pipeline_lasso['preprocess'].get_feature_names_out()

lasso_coef_table = pd.DataFrame({
    'feature': feature_names,
    'coefficient': coefs
})

top10    = lasso_coef_table.nlargest(10, 'coefficient')
bottom10 = lasso_coef_table.nsmallest(10, 'coefficient')
```
### Coefficients

| Feature                              | Coefficient |
|:-------------------------------------|------------:|
| genres_others                        | 2985.291907 |
| genres_Simulation                    |  899.312064 |
| publishers\_\['Fulqrum Publishing'\] |  772.646878 |
| estimated_owners_20000000 - 50000000 |  670.164379 |
| publishers\_\['SEGA'\]               |  486.142703 |
| release_year_2020                    |  433.267236 |
| estimated_owners_50000 - 100000      |  426.285499 |
| estimated_owners_5000000 - 10000000  |  426.152905 |
| release_year_2019                    |  413.313573 |
| genres_RPG                           |  331.760075 |
| publishers\_\['Kagura Games'\]       | -206.560163 |
| publishers\_\['Devolver Digital'\]   | -181.939496 |
| release_year_2009                    | -154.179173 |
| release_year_2024                    | -127.250029 |
| release_year_2010                    | -111.642857 |
| release_year_2012                    |  -71.802286 |
| release_year_2013                    |  -63.125205 |
| release_year_2023                    |  -47.393112 |
| publishers\_\['Square Enix'\]        |  -45.854015 |
| publishers_others                    |  -25.532638 |

The coefficient table above ranks the top positive and negative drivers of median gameplay time under our Elastic Net model (with 10 principal components retained for interpretability). On the positive side, games classified in the “others” genre enjoy the largest boost—adding roughly 2,985 minutes—while simulation titles contribute nearly 900 additional minutes relative to the reference categories. Published by Fulqrum Publishing corresponds to an increase of about 773 minutes, and titles with 20 million–50 million estimated owners add roughly 670 minutes. SEGA‐published games and titles released in 2020 each add over 400 minutes, as do games with 50 000–100 000 or 5 million–10 million owners. RPGs also feature among the top positive predictors, contributing around 332 minutes.

Conversely, several publishers and release years reduce expected gameplay time. Kagura Games titles subtract about 207 minutes, and Devolver Digital games subtract about 182 minutes. Older release years—2009 and 2010—reduce playtime by roughly 154 and 112 minutes, respectively, while more recent years (2012–2014 and 2023–2024) exert progressively smaller negative impacts (– 47 to – 127 minutes). Square Enix publications and the catch‑all “other” publisher category have modest negative coefficients (– 46 and – 26 minutes). Together, these coefficients illuminate which features most strongly influence player engagement as measured by median gameplay time.
```{python}
#| echo: false
df_pca = PCA().fit(transformed_X)
```

<<<<<<< HEAD

```{python PCA Scree plot}
#| echo: false
from matplotlib.ticker import MultipleLocator

fig, ax = plt.subplots()
sns.lineplot(
  x = np.arange(1, 64),
  y = df_pca.explained_variance_ratio_,
  ax = ax)
ax.set(
  xlabel = "Components",
  ylabel = "PVE",
  title = "Figure 1"
)

plt.show()
```


```{python PCA Cumulative pve}
plt.figure(figsize=(5,3))
plt.plot(np.arange(1, len(df_pca.explained_variance_ratio_)+1), np.cumsum(df_pca.explained_variance_ratio_), marker='o')
plt.xlabel('PC #'); plt.ylabel('Cumulative PVE')
plt.axhline(0.8, color='gray', linestyle='--')  # 80% threshold
plt.title('Figure 2')
plt.tight_layout()
plt.show()
```

### PCA
The scree plot (Figure 1) shows that the first principal component accounts for approximately 22 % of the variance in the preprocessed feature space, the second about 15 %, and the third about 3.5 %. Thereafter each additional component explains progressively less (the tenth component contributes roughly 1 %). When we examine the cumulative proportion of variance explained (PVE), the first five components capture about 55 % of the variance, and by ten components we reach roughly 78 % (just shy of our 80 % target). Although reaching 80 % would require around 13 components, we elected to proceed with 10 principal components as a representation that still captures the bulk of the structure in the data.

```{python}
#| echo: false
pca = PCA(n_components=3, random_state=42)
pca_scores = pca.fit_transform(transformed_X)

feature_names = preprocessor.get_feature_names_out()
pca = PCA(n_components=10, random_state=42)
pca_scores = pca.fit_transform(transformed_X)
#PCA Contributions
loadings = pd.DataFrame(
    pca.components_.T,
    index=feature_names,
    columns=[f"PC{i+1}" for i in range(pca.n_components_)]
)
```

```{python}
n = len(feature_names)

# build tick positions: every 5th up until n‑5, then the last 5 positions
every_fifth = list(range(0, n-5, 5))
last_five   = list(range(n-5, n))
tick_positions = sorted(set(every_fifth + last_five))

# plot
fig, ax = plt.subplots(figsize=(6, 12))
im = ax.imshow(loadings, aspect='auto')

# x ticks
ax.set_xticks(np.arange(loadings.shape[1]))
ax.set_xticklabels(loadings.columns, rotation=0)

# y ticks: only at our selected positions
ax.set_yticks(tick_positions)
ax.set_yticklabels(feature_names[tick_positions], fontsize=5, rotation=15, ha='right')

# colorbar and layout
fig.colorbar(im, ax=ax, label="Loading")
plt.title("PCA Loadings Heatmap")
plt.show()
```
PC 1 (Compatibility vs. Platform Restriction)
The strongest positive loading is on compatible_systems_3 (games available on three platforms), with a secondary positive contribution from peak_pgc (higher peak concurrent users). Games that run on more platforms and draw larger simultaneous audiences score highly on PC 1. Features like compatible_systems_2 and lower ownership bins load only weakly. PC 1 therefore differentiates broadly available, high‑traffic titles from more narrowly supported ones.

PC 2 (Two‑System vs. Three‑System Contrast)
PC 2 shows a large negative loading on compatible_systems_2 and a positive loading on compatible_systems_3. It effectively contrasts games that appear on exactly two platforms (low PC 2 scores) with those on three (high PC 2 scores). All other features load near zero, confirming that platform breadth is the sole driver of this component.

PC 3 (Engagement Intensity vs. Review Positivity)
The most negative loading is on peak_pgc, while positive_rate loads positively. High‑intensity titles (big peaks) receive low PC 3 scores, whereas games with high positive‑review rates score high. PC 3 thus separates “blockbuster” crowd‑pullers from smaller titles that nonetheless enjoy enthusiastic critical reception.

PC 4 (Low‑Ownership Tier Contrast)
A moderate positive loading appears on the 100 000–200 000 owners bin, with smaller contributions from adjacent bins. PC 4 distinguishes moderately owned games from both low‑ownership (< 50 000) and very high‑ownership (> 1 million) titles. This axis highlights variation in the “mid‑tail” of game popularity.

PC 5 (Mid‑Tail Ownership vs. Early Release Years)
PC 5 again loads positively on the 100 000–200 000 ownership bin, but also shows negative weight on very early release years (for example, release_year_2001 and release_year_2006). Titles in that mid‑ownership tier—but released more recently—score highly on PC 5, separating them from early‑era games with similar owner counts.

PC 6 (Adventure‑Genre Emphasis)
The dominant positive loading is on genres_Adventure, with smaller positive weights on a handful of publishers (for example, SEGA). PC 6 thus marks out adventure‑style titles in our dataset.

PC 7 and PC 8 (Sub‑Genre and Publisher Nuances)
PC 7 loads modestly on certain ownership bins and a couple of niche genres (for example, Racing). PC 8 shows a positive loading on owners_200 000–500 000 and a negative loading on peak_pgc, capturing another slice of mid‑range popularity versus raw engagement.

PC 9 (Release‑Year “2016 Spike”)
A strong positive loading on release_year_2016 indicates that games released in 2016 have unique covariation in our feature set—perhaps clustering around particular publishers or ownership profiles that year.

PC 10 (Release‑Year “2021 Effect”)
The largest loading is on release_year_2021, separating the newest cohort of games (with their own typical owner counts and review rates) from older titles.

Taken together, the first ten PCs reveal that platform compatibility, ownership tiers, engagement versus review balance, genre, and release‑year cohorts are the principal drivers of variation in our transformed feature space. By retaining these ten components (approximately 78 % of total variance), we capture the major structural patterns—platform scope, popularity strata, and temporal/genre effects—while discarding lower‑level noise.

### Regression Models

| Model       | Train MSE      | Test MSE       | Generalization Gap |
|------------:|---------------:|---------------:|-------------------:|
| OLS         | 1.1237 × 10⁷ | 1.6000 × 10⁷ | 4.7629 × 10⁶    |
| Lasso       | 1.1241 × 10⁷ | 1.6006 × 10⁷ | 4.7645 × 10⁶    |
| Elastic Net | 1.1252 × 10⁷ | 1.5996 × 10⁷ | 4.7447 × 10⁶    |
| PCA + LR    | 1.1482 × 10⁷ | 1.6165 × 10⁷ | 4.6835 × 10⁶    |

The results show that Elastic Net achieves the lowest test error (approximately 1.60 × 10⁷) despite a similar training error to OLS and Lasso, indicating its combined ℓ₁/ℓ₂ penalty provides a modest improvement in generalization. All models exhibit substantial overfitting, with generalization gaps on the order of 4.7 × 10⁶; PCA + LR reduces the gap slightly more than the others but at the cost of the highest test error, suggesting that discarding components also discards predictive signal. Lasso’s performance nearly mirrors that of OLS, implying that an ℓ₁ penalty alone was too aggressive. Overall, Elastic Net on the full feature set offers the best trade‑off between bias and variance for predicting median gameplay time.

```{python}
#| echo: false
pipeline_pca_lr = Pipeline([
    ('pre', preprocessor),
    ('pca', PCA(n_components=10, random_state=42)),
    ('lr', LinearRegression())
])


pipeline_pca_lr.fit(X_train, y_train)


y_train_pca_pred = pipeline_pca_lr.predict(X_train)
y_test_pca_pred = pipeline_pca_lr.predict(X_test)


mse_train_pca = mean_squared_error(y_train, y_train_pca_pred)
mse_test_pca = mean_squared_error(y_test, y_test_pca_pred)
gap_pca = mse_test_pca - mse_train_pca


results = [
    {'model': 'OLS', 'train_mse': 11237404.75, 'test_mse': 16000332.99},
    {'model': 'Lasso', 'train_mse': 11241401.01, 'test_mse': 16005884.32},
    {'model': 'Elastic Net', 'train_mse': enet_mse_train, 'test_mse' : enet_mse_test},
    {'model': 'PCA + LR', 'train_mse': mse_train_pca, 'test_mse': mse_test_pca}
]
```


### Random Forests
The Random Forest model achieved a test set Mean Squared Error (MSE) of 20,697,742.12, indicating moderate predictive performance. While the error value is relatively high in absolute terms, it remains informative given the scale and variance of the target variable median_playtime_forever in minutes.

In figure 5, the feature importance analysis reveals that peak_ccu (peak concurrent users) and positive_rate (user review rates) are the most influential predictors with a approxiamtely 0.2 importance level, suggesting that user activity and reception are primary drivers of the target outcome. Other important variables include price, release_year, estimated_owners(20000 - 50000, and 50000 - 100000), and game genres(Adventure). 

```{python, top 10 predictors}
#| echo: false
# bar plot for top 10 predictors
feature_names = pipeline_rf.named_steps['preprocess'].get_feature_names_out()
importances = pipeline_rf.named_steps['rf'].feature_importances_

top_ten = 10
top_idx = importances.argsort()[-top_ten:][::-1]

plt.figure(figsize=(10,6))
sns.barplot(x=importances[top_idx], y=feature_names[top_idx])
plt.title("Figure 5: Top 10 Predictors by Random Forest")
plt.xlabel("Importance")
plt.ylabel("Variables")
plt.tight_layout()
plt.show()
```



## Discussion

### Limitations

Our project also has several important limitations that should be acknowledged. First, many free-to-play games that became extremely popular on Steam rely heavily on in-game purchases, which are not reflected in our dataset. For example, Counter-Strike: Global Offensive (CSGO) is one of the most widely played FPS games on Steam and is free to download. However, players often spend significant amounts of money on cosmetic weapon skins, some of which feature animated effects and can cost hundreds of dollars. Our analysis does not account for this form of monetization strategy, which plays a crucial role in the game's success and revenue model.

Second, Steam is not the only platform where games are sold or played. Many titles are cross-platform and also available on consoles such as the Nintendo Switch, Xbox, or PlayStation, or through other digital stores like the Epic Games Store. Players may choose different platforms based on their preferences or hardware availability. Therefore, evaluating a game’s success solely based on Steam data presents an incomplete picture, as it ignores potentially large portions of the user base and sales figures.

Third, during our data cleaning process, we computed the positive rating ratio by dividing the number of positive reviews by the total number of reviews. However, this method can be misleading for games with very few reviews, sometimes only one or two. In such cases, a single review can skew the ratio to 100% or 0%, which does not meaningfully reflect the broader sentiment of the player community.

### Future Work

For future improvement, the first thing we can improve on is feature engineering. We could incorporate the total number of reviews as a separate feature and have a more careful approach in dealing with categorical variables such as publishers and game genres. We also consider adding interaction terms and filtering out all games that have in-game purchases. What's more, we believe implementing more non-linear model will yield better results.

### Ethical Considerations

Regarding ethical considerations, we now recognize that even our decision to treat “positive reviews” as a key variable could be problematic. Games with LGBTQ+ themes, female protagonists, or minority developers are often subject to review bombing by toxic communities. A well-known example is the indie game Celeste, whose developer is a transgender woman. Despite receiving critical acclaim and winning awards for gameplay and storytelling, the game was targeted with waves of negative reviews due to the developer’s identity. If our model interprets low review scores as an indicator of poor game quality without context, we risk reproducing and legitimizing cultural bias through algorithmic analysis.

## Conclusions

## References

Martin Bustos Roman. (2022). Steam Games Dataset \[Data set\]. Kaggle. https://doi.org/10.34740/KAGGLE/DS/2109585

Nikitin, K. (2024). *Finnish game developers in transforming the industry* (Master’s thesis, University of Vaasa). University of Vaasa Institutional Repository.

Polcyn, S. (2019). The evolution of free time throughout history. Video games as a modern leisure activity. Biuletyn Historii Wychowania, 38, 187–200. https://doi.org/10.14746/bhw.2018.38.12
